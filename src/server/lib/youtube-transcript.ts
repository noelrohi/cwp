import { nanoid } from "nanoid";
import { Innertube } from "youtubei.js";
import type { TranscriptUtterance } from "@/types/transcript";

export type YouTubeTranscriptSegment = {
  text: string;
  startMs: number;
  durationMs: number;
};

export type YouTubeTranscriptResult = {
  segments: YouTubeTranscriptSegment[];
  language: string;
  isAutoGenerated: boolean;
  duration: number;
};

/**
 * Fetch transcript from YouTube using Innertube
 */
export async function fetchYouTubeTranscript(
  videoId: string,
): Promise<YouTubeTranscriptResult | null> {
  try {
    const youtube = await Innertube.create();
    const videoInfo = await youtube.getInfo(videoId);

    // Check if captions are available
    const transcriptData = await videoInfo.getTranscript();

    if (!transcriptData) {
      console.log(`No transcript available for video: ${videoId}`);
      return null;
    }

    // Extract segments safely
    const segments: YouTubeTranscriptSegment[] = [];
    const initialSegments =
      transcriptData.transcript?.content?.body?.initial_segments;

    if (!initialSegments || !Array.isArray(initialSegments)) {
      console.log(`No transcript segments found for video: ${videoId}`);
      return null;
    }

    for (const segment of initialSegments) {
      // Type guard to check if segment has the expected properties
      if (
        segment &&
        typeof segment === "object" &&
        "snippet" in segment &&
        segment.snippet &&
        typeof segment.snippet === "object" &&
        "text" in segment.snippet &&
        "start_ms" in segment &&
        "end_ms" in segment
      ) {
        const text = segment.snippet.text;
        // YouTube API returns start_ms and end_ms as strings, need to parse them
        const startMs = Number(segment.start_ms);
        const endMs = Number(segment.end_ms);

        if (
          typeof text === "string" &&
          !Number.isNaN(startMs) &&
          !Number.isNaN(endMs)
        ) {
          segments.push({
            text,
            startMs,
            durationMs: endMs - startMs,
          });
        }
      }
    }

    if (segments.length === 0) {
      console.log(`No valid segments extracted for video: ${videoId}`);
      return null;
    }

    return {
      segments,
      language: "en",
      isAutoGenerated: true,
      duration: videoInfo.basic_info.duration || 0,
    };
  } catch (error) {
    console.error(`Failed to fetch YouTube transcript for ${videoId}:`, error);
    return null;
  }
}

/**
 * Transform YouTube transcript format to match Deepgram utterance format
 * This ensures compatibility with existing chunking and processing logic
 */
export function transformYouTubeTranscript(
  transcript: YouTubeTranscriptResult,
): TranscriptUtterance[] {
  const utterances: TranscriptUtterance[] = [];
  let currentUtterance: Partial<TranscriptUtterance> | null = null;
  const UTTERANCE_GAP_THRESHOLD_MS = 1500; // Group segments within 1.5s into same utterance

  for (let i = 0; i < transcript.segments.length; i++) {
    const segment = transcript.segments[i];
    const startSec = segment.startMs / 1000;
    const endSec = (segment.startMs + segment.durationMs) / 1000;

    // Split segment text into individual words
    const words = segment.text.trim().split(/\s+/);
    const durationPerWord =
      words.length > 0 ? (endSec - startSec) / words.length : 0;

    // Create individual word objects for proper chunking
    const wordObjects = words.map((wordText, wordIdx) => ({
      word: wordText,
      start: startSec + wordIdx * durationPerWord,
      end: startSec + (wordIdx + 1) * durationPerWord,
      confidence: 1.0,
      punctuated_word: wordText,
      speaker: 0,
    }));

    // Check if we should start a new utterance
    const shouldStartNew =
      !currentUtterance ||
      (i > 0 &&
        segment.startMs -
          (transcript.segments[i - 1].startMs +
            transcript.segments[i - 1].durationMs) >
          UTTERANCE_GAP_THRESHOLD_MS);

    if (shouldStartNew) {
      if (currentUtterance?.words) {
        // Complete the current utterance with all required fields
        utterances.push({
          start: currentUtterance.start!,
          end: currentUtterance.end!,
          confidence: 1.0,
          channel: 0,
          transcript: currentUtterance.transcript!,
          words: currentUtterance.words,
          speaker: 0,
          id: nanoid(),
        });
      }
      currentUtterance = {
        start: startSec,
        end: endSec,
        transcript: segment.text,
        words: [...wordObjects],
      };
    } else if (currentUtterance) {
      // Extend current utterance
      currentUtterance.end = endSec;
      currentUtterance.transcript += ` ${segment.text}`;
      currentUtterance.words!.push(...wordObjects);
    }
  }

  // Push the last utterance
  if (currentUtterance?.words) {
    utterances.push({
      start: currentUtterance.start!,
      end: currentUtterance.end!,
      confidence: 1.0,
      channel: 0,
      transcript: currentUtterance.transcript!,
      words: currentUtterance.words,
      speaker: 0,
      id: nanoid(),
    });
  }

  return utterances;
}

/**
 * Fetch and transform YouTube transcript in one call
 */
export async function getYouTubeTranscriptUtterances(
  videoId: string,
): Promise<TranscriptUtterance[] | null> {
  const transcript = await fetchYouTubeTranscript(videoId);
  if (!transcript) {
    return null;
  }
  return transformYouTubeTranscript(transcript);
}
